{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch as T\n",
    "import imageio\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "from IPython.display import HTML\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "boardSize = (12,15)   # Legt die Größe des Feldes fest\n",
    "safeDist = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showhist_map(hist):\n",
    "        for i in range(len(hist)):\n",
    "            if all(elm < boardSize[0] for elm in hist[int(i)]):\n",
    "                h = hist[i].astype(np.int)\n",
    "                \n",
    "                if np.max(hist[i]) == 1: \n",
    "                    plt.imshow(hist[i].reshape(boardSize[0], boardSize[1]), cmap='hot', interpolation='nearest')\n",
    "                else:\n",
    "                    plt.imshow(np.zeros((boardSize[0], boardSize[1])))\n",
    "                plt.show()\n",
    "                \n",
    "                \n",
    "            \n",
    "def showReward(h,elemMax=True):\n",
    "    \n",
    "    m = np.zeros(boardSize)\n",
    "    for i in range(boardSize[0]):\n",
    "        for j in range(boardSize[1]):\n",
    "            m[i,j] , _= env.getRewardForField(i,j)\n",
    "            if(m[i,j] > 30):\n",
    "                if elemMax:\n",
    "                    m[i,j] = 10\n",
    "    \n",
    "    return m.astype(np.int)\n",
    "\n",
    "\n",
    "def fig2data ( fig ):\n",
    "    fig.canvas.draw ( )\n",
    "\n",
    "    w,h = fig.canvas.get_width_height()\n",
    "\n",
    "    buf = np.frombuffer ( fig.canvas.tostring_rgb(), dtype=np.uint8 )\n",
    "    buf.shape = ( h, w,3 )\n",
    " \n",
    "    buf = np.roll ( buf, 3, axis = 2 )\n",
    "    plt.close()\n",
    "    return buf\n",
    "\n",
    "def showhist_ani(hist):\n",
    "    frame_array = []\n",
    "    for i in range(len(hist)):\n",
    "        if all(elm < boardSize[0] for elm in hist[int(i)]):\n",
    "            h = hist[i].astype(np.int)\n",
    "            fig = plt.figure()\n",
    "            if np.max(hist[i]) == 1: \n",
    "                plt.imshow(hist[i].reshape(boardSize[0], boardSize[1]), cmap='hot', interpolation='nearest')\n",
    "            else:\n",
    "                plt.imshow(np.zeros((boardSize[0], boardSize[1])))\n",
    "            frame_array.append(fig2data ( fig ))\n",
    "    #\n",
    "\n",
    "    w = imageio.get_writer('output.mp4', fps = 6,quality=6)\n",
    "    for i in range(len(frame_array)):\n",
    "        w.append_data(frame_array[i] )\n",
    "    w.close()\n",
    "    #return frame_array\n",
    "\n",
    "def playVideo(path):\n",
    "    before = \"\"\"<video width=\"864\" height=\"576\" controls><source src=\"\"\"\n",
    "    end = \"\"\" type=\"video/mp4\"></video>\"\"\"\n",
    "    return HTML(before + path + end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, max_size, input_shape):\n",
    "        self.mem_size = max_size\n",
    "        self.mem_cntr = 0\n",
    "        self.state_memory = np.zeros((self.mem_size, *input_shape),\n",
    "                                    dtype=np.float32)\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *input_shape),\n",
    "                                        dtype=np.float32)\n",
    "        self.action_memory = np.zeros(self.mem_size, dtype=np.int64)\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.uint8)\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "        self.terminal_memory[index] = done\n",
    "        self.mem_cntr += 1\n",
    "\n",
    "    def sample_buffer(self, batch_size):\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "        batch = np.random.choice(max_mem, batch_size, replace=False)\n",
    "\n",
    "        states = self.state_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        states_ = self.new_state_memory[batch]\n",
    "        terminal = self.terminal_memory[batch]\n",
    "\n",
    "        return states, actions, rewards, states_, terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingLinearDeepQNetwork(nn.Module):\n",
    "    def __init__(self, ALPHA, n_actions, name, input_dims, chkpt_dir=''):\n",
    "        super(DuelingLinearDeepQNetwork, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(*input_dims, 256)\n",
    "        self.fc2 = nn.Linear(256, 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        \n",
    "        self.preV = nn.Linear(256, 128)\n",
    "        self.V = nn.Linear(128, 1)\n",
    "        \n",
    "        self.preA = nn.Linear(256,128)\n",
    "        self.A = nn.Linear(128, n_actions)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=ALPHA)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "        self.checkpoint_dir = chkpt_dir\n",
    "        self.checkpoint_file = os.path.join(self.checkpoint_dir, name+'_dqn')\n",
    "\n",
    "    def forward(self, state):\n",
    "        l1 = F.relu(self.fc1(state))\n",
    "        l2 = F.relu(self.fc2(l1))\n",
    "        l3 = F.relu(self.fc3(l2))\n",
    "        prV = F.relu(self.preV(l3))\n",
    "        V = self.V(prV)\n",
    "        prA = F.relu(self.preA(l3))\n",
    "        A = self.A(prA)\n",
    "\n",
    "        return V, A\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        #print('... saving checkpoint ...')\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        #print('... loading checkpoint ...')\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, gamma, epsilon, lr, n_actions, input_dims,\n",
    "                 mem_size, batch_size, eps_min=0.01, eps_dec=5e-7,\n",
    "                 replace=1000, chkpt_dir=''):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.lr = lr\n",
    "        self.n_actions = n_actions\n",
    "        self.input_dims = input_dims\n",
    "        self.batch_size = batch_size\n",
    "        self.eps_min = eps_min\n",
    "        self.eps_dec = eps_dec\n",
    "        self.replace_target_cnt = replace\n",
    "        self.chkpt_dir = chkpt_dir\n",
    "        self.action_space = [i for i in range(self.n_actions)]\n",
    "        self.learn_step_counter = 0\n",
    "        print(mem_size)\n",
    "        print(input_dims)\n",
    "        print(n_actions)\n",
    "        self.memory = ReplayBuffer(mem_size, input_dims)\n",
    "\n",
    "        self.q_eval = DuelingLinearDeepQNetwork(self.lr, self.n_actions,\n",
    "                                   input_dims=self.input_dims,\n",
    "                                   name='model',\n",
    "                                   chkpt_dir=self.chkpt_dir)\n",
    "\n",
    "        self.q_next = DuelingLinearDeepQNetwork(self.lr, self.n_actions,\n",
    "                                   input_dims=self.input_dims,\n",
    "                                   name='model_next',\n",
    "                                   chkpt_dir=self.chkpt_dir)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        if np.random.random() > self.epsilon:\n",
    "            state = T.tensor([observation],dtype=T.float).to(self.q_eval.device)\n",
    "            _, advantage = self.q_eval.forward(state)\n",
    "            action = T.argmax(advantage).item()\n",
    "        else:\n",
    "            action = np.random.choice(self.action_space)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        self.memory.store_transition(state, action, reward, state_, done)\n",
    "\n",
    "    def replace_target_network(self):\n",
    "        if self.learn_step_counter % self.replace_target_cnt == 0:\n",
    "            self.q_next.load_state_dict(self.q_eval.state_dict())\n",
    "\n",
    "    def decrement_epsilon(self):\n",
    "        self.epsilon = self.epsilon - self.eps_dec \\\n",
    "                        if self.epsilon > self.eps_min else self.eps_min\n",
    "\n",
    "    def save_models(self):\n",
    "        self.q_eval.save_checkpoint()\n",
    "        self.q_next.save_checkpoint()\n",
    "\n",
    "    def load_models(self):\n",
    "        self.q_eval.load_checkpoint()\n",
    "        self.q_next.load_checkpoint()\n",
    "\n",
    "    def learn(self):\n",
    "        if self.memory.mem_cntr < self.batch_size:\n",
    "            return\n",
    "\n",
    "        self.q_eval.optimizer.zero_grad()\n",
    "\n",
    "        self.replace_target_network()\n",
    "\n",
    "        state, action, reward, new_state, done = \\\n",
    "                                self.memory.sample_buffer(self.batch_size)\n",
    "\n",
    "        states = T.tensor(state).to(self.q_eval.device)\n",
    "        rewards = T.tensor(reward).to(self.q_eval.device)\n",
    "        dones = T.tensor(done).to(self.q_eval.device)\n",
    "        actions = T.tensor(action).to(self.q_eval.device)\n",
    "        states_ = T.tensor(new_state).to(self.q_eval.device)\n",
    "\n",
    "        indices = np.arange(self.batch_size)\n",
    "\n",
    "        V_s, A_s = self.q_eval.forward(states)\n",
    "        V_s_, A_s_ = self.q_next.forward(states_)\n",
    "\n",
    "        V_s_eval, A_s_eval = self.q_eval.forward(states_)\n",
    "\n",
    "        q_pred = T.add(V_s,\n",
    "                        (A_s - A_s.mean(dim=1, keepdim=True)))[indices, actions]\n",
    "        q_next = T.add(V_s_,\n",
    "                        (A_s_ - A_s_.mean(dim=1, keepdim=True)))\n",
    "\n",
    "        q_eval = T.add(V_s_eval, (A_s_eval - A_s_eval.mean(dim=1,keepdim=True)))\n",
    "\n",
    "        max_actions = T.argmax(q_eval, dim=1)\n",
    "\n",
    "        q_next[dones] = 0.0\n",
    "        q_target = rewards + self.gamma*q_next[indices, max_actions]\n",
    "\n",
    "        loss = self.q_eval.loss(q_target, q_pred).to(self.q_eval.device)\n",
    "        loss.backward()\n",
    "        self.q_eval.optimizer.step()\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "        self.decrement_epsilon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class game:\n",
    "    def __init__(self, startPos, aimPos ,obstacles ,boardSize = boardSize ):\n",
    "        self.startPos = np.copy(startPos)\n",
    "        self.playerPos = startPos\n",
    "        self.aim = aim(aimPos[0],aimPos[1])\n",
    "        self.boardSize = boardSize\n",
    "        self.obstacles = obstacles\n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "        self.playerPos = self.startPos\n",
    "        self.reward = 0\n",
    "\n",
    "        return self.createMap().reshape(boardSize[0]*boardSize[1])   \n",
    "        \n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        \n",
    "        if(action == 0):\n",
    "            self.playerPos[0] +=1\n",
    "        if(action == 1):\n",
    "            self.playerPos[0] -=1\n",
    "        if(action == 2):\n",
    "            self.playerPos[1] +=1\n",
    "        if(action == 3):\n",
    "            self.playerPos[1] -=1\n",
    "        \n",
    "        \n",
    "        observ = self.createMap().reshape(boardSize[0]*boardSize[1])\n",
    "        \n",
    "        reward, done = self.getRewardForField(self.playerPos[0],self.playerPos[1])\n",
    "        \n",
    "        return observ, reward, done\n",
    "            \n",
    "    def createMap(self,plot=False):\n",
    "        \n",
    "        m = np.zeros(self.boardSize)\n",
    "        for ob in self.obstacles:\n",
    "            m[ob.x,ob.y] = 0.3\n",
    "        m[self.aim.x,self.aim.y] = 0.9\n",
    "        \n",
    "        if(self.playerPos[0]>0 and self.playerPos[0]< boardSize[0] and\n",
    "          self.playerPos[1]>0 and self.playerPos[1]< boardSize[1]):\n",
    "            m[self.playerPos[0],self.playerPos[1]] = 1\n",
    "\n",
    "        if plot:\n",
    "            plt.imshow(m, cmap='hot', interpolation='nearest')\n",
    "            plt.show()\n",
    "        return m\n",
    "    \n",
    "    def checkBounds(self,p):\n",
    "        if(p[0]<0):\n",
    "            return -10000, True\n",
    "        if(p[1]<0):\n",
    "            return -10000, True\n",
    "        if(p[0]>=boardSize[0]):\n",
    "            return -10000, True\n",
    "        if(p[1]>=boardSize[1]):\n",
    "            return -10000, True\n",
    "        return 0, False\n",
    "    \n",
    "    def distance(self, a, b,special=False):\n",
    "        if(special):\n",
    "            return np.sqrt(np.square(a[0]-b.x)+np.square(a[1]-b.y))\n",
    "        else:\n",
    "            return np.sqrt(np.square(a.x-b.x)+np.square(a.y-b.y))\n",
    "        \n",
    "    def getRewardForField(self, x, y): \n",
    "        done = False\n",
    "        pos = [x,y]\n",
    "        reward = 0\n",
    "        reward -= 100*(self.distance(pos,self.aim,special=True))/(boardSize[1]+boardSize[0])**2\n",
    "        rew , done = self.checkBounds(pos)\n",
    "        reward += rew\n",
    "        for ob in self.obstacles:\n",
    "            reward -= 1000*np.exp(-(self.distance(pos,ob,special=True)*safeDist))\n",
    "        \n",
    "        edgeControl = 600\n",
    "        if(pos[0]==0 or pos[0]== boardSize[0]-1):\n",
    "            reward -= edgeControl\n",
    "        if(pos[1]==0 or pos[1]== boardSize[1]-1):\n",
    "            reward -= edgeControl\n",
    "        if(x == self.aim.x and y == self.aim.y):\n",
    "            reward =  MAX_REWARD\n",
    "            done = True\n",
    "        return reward, done\n",
    "\n",
    "class aim:\n",
    "    def __init__(self,x,y):\n",
    "        self.x = x\n",
    "        self.y = y        \n",
    "\n",
    "        \n",
    "class obstacle:\n",
    "    def __init__(self,x,y):\n",
    "        self.x = x\n",
    "        self.y = y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "[180]\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 250/10000 [01:32<1:46:58,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  249 score: -6788.02  average score -6760.62 Epsilon 0.525 Erreicht: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 368/10000 [02:34<1:07:16,  2.39it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-9763bae8691d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m                                     reward, observation_, int(done))\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobservation_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-49-2f75f39bdbac>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn_step_counter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##### Here\n",
    "\n",
    "MAX_REWARD = 20000\n",
    "#gameStates = np.zeros((timeSteps, boardSize[0],boardSize[1] ))\n",
    "\n",
    "agent = Agent(gamma=0.99, epsilon=1.0, lr=1*5e-3, n_actions=4, input_dims=[boardSize[0]*boardSize[1]], \n",
    "              mem_size=100000, batch_size=64, eps_min=0.01, eps_dec=5*1e-5, replace=100)\n",
    "load_checkpoint = False\n",
    "\n",
    "if load_checkpoint:\n",
    "        agent.load_models()\n",
    "        \n",
    "scores, eps_hist = [], []\n",
    "n_games = 10000\n",
    "\n",
    "num_obstaces = np.random.randint(15,25)\n",
    "obstacles = []\n",
    "for i in range(num_obstaces):\n",
    "    obstacles.append(obstacle(np.random.randint(1,boardSize[0]),np.random.randint(1,boardSize[1])))\n",
    "    \n",
    "score_saver = []\n",
    "avg_score_saver = []\n",
    "ddqn_scores = []\n",
    "eps_history = []\n",
    "savedGames = []\n",
    "MAX_ITER = 60\n",
    "prec = 40\n",
    "reached = 0\n",
    "reached_last_100 = 0\n",
    "for i in tqdm(range(n_games)):\n",
    "    score = 0\n",
    "    done = False\n",
    "    aimPos = np.array([np.random.randint(6,boardSize[0]-2), np.random.randint(int(boardSize[1]/2+2),boardSize[1]-2)])\n",
    "    playerpos = np.array([np.random.randint(2,4),np.random.randint(2,5)])            \n",
    "    env = game(playerpos,aimPos,obstacles)\n",
    "    observation = env.reset()\n",
    "    \n",
    "\n",
    "    game_sav = []\n",
    "    iteration = 0\n",
    "    while not done:\n",
    "        iteration +=1\n",
    "\n",
    "        action = agent.choose_action(observation)\n",
    "        observation_, reward, done = env.step(action)\n",
    "        if reward == MAX_REWARD:\n",
    "            reached += 1\n",
    "            if i > (n_games -100):\n",
    "                reached_last_100 += 1\n",
    "                \n",
    "        score += reward\n",
    "        agent.store_transition(observation, action,\n",
    "                                    reward, observation_, int(done))\n",
    "        \n",
    "        agent.learn()\n",
    "        observation = observation_\n",
    "\n",
    "        game_sav.append(observation_)\n",
    "        eps_history.append(agent.epsilon)\n",
    "\n",
    "        ddqn_scores.append(score)\n",
    "        if(i > 20):\n",
    "            avg_score = np.mean(ddqn_scores[-10])\n",
    "        \n",
    "        if iteration == MAX_ITER:\n",
    "            done = True\n",
    "\n",
    "        if i % 10 == 0 and i > 0:\n",
    "            agent.save_models()\n",
    "    score_saver.append(score)\n",
    "    if(i > 20):\n",
    "        avg_score_saver.append(avg_score)\n",
    "        if i % int(n_games/prec) == int(n_games/prec)-1:\n",
    "            print('episode: ', i,'score: %.2f' % score,\n",
    "                  ' average score %.2f' % avg_score,\n",
    "                  'Epsilon %.3f' % agent.epsilon,\n",
    "                  'Erreicht: ' +str(reached)) \n",
    "    savedGames.append(game_sav)\n",
    "    \n",
    "   \n",
    "print(\"\")\n",
    "print(str(n_games) + \" Spieldurchläufe: \"  +str(reached) + \" mal Ziel erreicht, Quote = \" + str(reached/n_games))\n",
    "print(\"Quote der letzten 100 Durchläufe \" + str(reached_last_100/100) )\n",
    "plt.plot(score_saver)\n",
    "plt.show()\n",
    "plt.plot(avg_score_saver)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " plt.plot(score_saver[-300:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(showReward(savedGames[-1][0],elemMax=False)).astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(showReward(savedGames[-2][0]).reshape(boardSize[0], boardSize[1]), cmap='hot', interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_array = showhist_ani(savedGames[-np.random.randint(1,10)])\n",
    "playVideo('output.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
